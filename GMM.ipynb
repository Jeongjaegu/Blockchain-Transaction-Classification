{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running GMM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imorting packages and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#%matplotlib inline\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import edward as ed\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "import six\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from edward.models import Categorical\n",
    "\n",
    "#from Models.GMM.Save import simple_save\n",
    "\n",
    "from edward.models import (\n",
    "    Categorical, Dirichlet, Empirical, InverseGamma,\n",
    "    MultivariateNormalDiag, Normal, ParamMixture, RandomVariable, Bernoulli)\n",
    "from tensorflow.contrib.distributions import Distribution\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "\n",
    "###########\n",
    "#For saving models:\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.saved_model import builder\n",
    "from tensorflow.python.saved_model import signature_constants\n",
    "from tensorflow.python.saved_model import signature_def_utils\n",
    "from tensorflow.python.saved_model import tag_constants\n",
    "#from tensorflow.python.util.tf_export import tf_export\n",
    "\n",
    "#Reading data:\n",
    "subset= pd.read_csv('\\user_rec.csv', sep=\",\", header=0)\n",
    "x_trainn=subset.values                                                  #Redefine x_train into an array instead of dataframe\n",
    "\n",
    "#Standardizing:\n",
    "from sklearn import preprocessing\n",
    "X= preprocessing.scale(x_trainn)\n",
    "\n",
    "\n",
    "#Defining test & train:\n",
    "x_train= X[0:4984,]                                                  #Defining training data\n",
    "x_test= X[4984:,]                                                   #Defining test data\n",
    "N_testt= len(x_test)                                                    #Defining the length of the test-set\n",
    "\n",
    "\n",
    "N = len(x_train)  # number of data points                               #Setting parameters - N is defined from the number of rows\n",
    "K = 30  # number of components                                           #Setting parameters - number of clusters\n",
    "D = x_train.shape[1]  # dimensionality of data                           #Setting parameters - dimension of data\n",
    "ed.set_seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definerer modellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Model:\n",
    "pi = Dirichlet(tf.ones(K))                                              \n",
    "                                                                        \n",
    "mu = Normal(tf.zeros(D), tf.ones(D), sample_shape=K)                    \n",
    "sigmasq = InverseGamma(tf.ones(D), tf.ones(D), sample_shape=K)          \n",
    "                                                                        \n",
    "x = ParamMixture(pi, {'loc': mu, 'scale_diag': tf.sqrt(sigmasq)},       \n",
    "                 MultivariateNormalDiag,                                \n",
    "                 sample_shape=N)\n",
    "z = x.cat                                                               \n",
    "\n",
    "\n",
    "#Inference:                                                             \n",
    "T = 800                                                                \n",
    "qpi = Empirical(                                                       \n",
    "    tf.get_variable(                                                   \n",
    "    \"qpi/params\", [T, K],                                              \n",
    "    initializer=tf.constant_initializer(1.0 / K)))                     \n",
    "qmu = Empirical(tf.get_variable(                                       \n",
    "    \"qmu/params\", [T, K, D],\n",
    "    initializer=tf.zeros_initializer()))\n",
    "qsigmasq = Empirical(tf.get_variable(                                  \n",
    "    \"qsigmasq/params\", [T, K, D],\n",
    "    initializer=tf.ones_initializer()))\n",
    "qz = Empirical(tf.get_variable(                                        \n",
    "    \"qz/params\", [T, N],\n",
    "    initializer=tf.zeros_initializer(),\n",
    "    dtype=tf.int32))\n",
    "\n",
    "#Running Gibbs Sampling:\n",
    "inference = ed.Gibbs({pi: qpi, mu: qmu, sigmasq: qsigmasq, z: qz},     \n",
    "                     data={x: x_train})                                 \n",
    "inference.initialize()                                                  \n",
    "\n",
    "sess = ed.get_session()\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "t_ph = tf.placeholder(tf.int32,[])                                      \n",
    "running_cluster_means = tf.reduce_mean(qmu.params[:t_ph], 0)            \n",
    "\n",
    "for _ in range(inference.n_iter):\n",
    "  info_dict = inference.update()\n",
    "  inference.print_progress(info_dict)\n",
    "  t = info_dict['t']\n",
    "  if t % inference.n_print == 0:\n",
    "    print(\"\\nInferred cluster means:\")\n",
    "    print(sess.run(running_cluster_means, {t_ph: t - 1}))                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criticism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Train: Sampling from posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Criticism:\n",
    "# Calculate likelihood for each data point and cluster assignment,\n",
    "# averaged over many posterior samples. `x_post` has shape (N, 100, K, D).\n",
    "pi_sample = qpi.sample(100)\n",
    "mu_sample = qmu.sample(100)                                                 \n",
    "sigmasq_sample = qsigmasq.sample(100)                                       \n",
    "x_post = Normal(loc=tf.ones([N, 1, 1, 1]) * mu_sample,                      \n",
    "                scale=tf.ones([N, 1, 1, 1]) * tf.sqrt(sigmasq_sample))      \n",
    "                                                                            \n",
    "                                                                            \n",
    "x_broadcasted = tf.tile(tf.reshape(x_train, [N, 1, 1, D]), [1, 100, K, 1])  \n",
    "x_broadcasted = tf.cast(x_broadcasted, tf.float32)                          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train: Running log_liks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Log_liks:\n",
    "log_liks = x_post.log_prob(x_broadcasted)                                   \n",
    "log_liks = tf.reduce_sum(log_liks, 3)                                       \n",
    "log_liks = tf.add(log_liks,tf.log(pi_sample))                               \n",
    "log_liks = tf.subtract(tf.reduce_logsumexp(log_liks, 1),tf.log(100.0))      \n",
    "log_dens=tf.reduce_logsumexp(log_liks,1)                                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Train:  Samlet log_liks & cluster assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reduce_sum(log_dens).eval()                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#cluster with the highest likelihood for each data point assigned cluster value.\n",
    "clusters = tf.argmax(log_liks, 1).eval()                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test: Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Prediction on test data\n",
    "x_broadcasted = tf.tile(tf.reshape(x_test, [N, 1, 1, D]), [1, 100, K, 1]) \n",
    "x_broadcasted = tf.cast(x_broadcasted, tf.float32)                        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test: Log_liks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Log_liks:\n",
    "log_liks = x_post.log_prob(x_broadcasted)                                   \n",
    "log_liks = tf.reduce_sum(log_liks, 3)                                       \n",
    "log_liks = tf.add(log_liks,tf.log(pi_sample))                               \n",
    "log_liks = tf.subtract(tf.reduce_logsumexp(log_liks, 1),tf.log(100.0))      \n",
    "log_dens=tf.reduce_logsumexp(log_liks,1)                                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test: Samplet log_liks og cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reduce_sum(log_dens).eval()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Cluster assignments:\n",
    "clusters = tf.argmax(log_liks, 1).eval()     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
